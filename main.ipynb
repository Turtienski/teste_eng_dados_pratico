{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste Prático para Engenheiros de Dados - Itaú RTDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa libs necessárias\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "load_dotenv()\n",
    "\n",
    "import requests as req\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#import pytest\n",
    "import unittest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ETL e Manipulação de Dados\n",
    "\n",
    "Utilize o arquivo sales_data.csv e:\n",
    "Limpe os dados removendo linhas duplicadas e tratando valores ausentes.\n",
    "Transforme o valor da venda de uma moeda fictícia para USD usando a taxa de conversão de 1 FICT = 0.75 USD.\n",
    "Carregue os dados limpos e transformados em um banco de dados relacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataframe original tem 40 linhas.\n",
      "O novo dataframe tem 40 linhas.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'SHWAAEC74RQ3689R',\n",
       "  'HostId': 'sXp61PSBR8iu+gITUeFwVBLHC9sN/8qvBspE0lQKDGq+7e0QRwI92L6T0bD/wQjjul+pflY26Ww=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'sXp61PSBR8iu+gITUeFwVBLHC9sN/8qvBspE0lQKDGq+7e0QRwI92L6T0bD/wQjjul+pflY26Ww=',\n",
       "   'x-amz-request-id': 'SHWAAEC74RQ3689R',\n",
       "   'date': 'Mon, 07 Aug 2023 16:18:26 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"5fbc678193a6c41a0b5cf6dd103a82e1\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"5fbc678193a6c41a0b5cf6dd103a82e1\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Implementação estrutural\n",
    "\n",
    "# Passo 1: Extração e Limpeza dos Dados\n",
    "# Carrega o arquivo CSV\n",
    "df_sales_raw = pd.read_csv('input/sales_data.csv')\n",
    "\n",
    "# Conta o número de linhas após as transformações\n",
    "print(\"O dataframe original tem\",len(df_sales_raw.index),\"linhas.\")\n",
    "\n",
    "# Remove linhas duplicadas\n",
    "df_sales_refined = df_sales_raw.drop_duplicates()\n",
    "\n",
    "# Tratar valores ausentes (caso existam)\n",
    "# Substitui valores ausentes por zero.\n",
    "df_sales_refined = df_sales_refined.fillna(0)\n",
    "\n",
    "# Passo 2: Transformação dos Dados\n",
    "# Converter o valor da venda de moeda fictícia (FICT) para USD usando a taxa de conversão de 1 FICT = 0.75 USD\n",
    "df_sales_refined['usd_sale_value'] = df_sales_refined['sale_value'] * 0.75\n",
    "\n",
    "# Conta o número de linhas após as transformações\n",
    "print(\"O novo dataframe tem\",len(df_sales_refined.index),\"linhas.\")\n",
    "\n",
    "#df_sales_refined.to_csv(\"output/struct_sales_data.csv\", index=False)\n",
    "\n",
    "json_sales_refined = df_sales_refined.to_json()\n",
    "\n",
    "aws_access_key_id = getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = getenv(\"AWS_SECRET_KEY\")\n",
    "    \n",
    "s3 = boto3.client(\n",
    "    's3'\n",
    "    , aws_access_key_id=aws_access_key_id\n",
    "    , aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "bucket_name = 'bucket-teste-iam'\n",
    "file_name = 'json_sales_refined.json'\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=file_name, Body=json_sales_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Análise com Apache Spark(utilize PySpark ou Spark)\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "Dado um conjunto fictício de logs `website_logs.csv`:\n",
    "\n",
    "- Identifique as 10 páginas mais visitadas.\n",
    "- Calcule a média de duração das sessões dos usuários.\n",
    "- Determine quantos usuários retornam ao site mais de uma vez por semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria sessão Spark\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .master(\"local[1]\")\n",
    "                     .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------------+----------+\n",
      "|user_id|         page_url|session_duration|      date|\n",
      "+-------+-----------------+----------------+----------+\n",
      "|  10001|    homepage.html|            15.0|2023-07-25|\n",
      "|  10002|product_page.html|           120.0|2023-07-25|\n",
      "|  10003|    checkout.html|            45.0|2023-07-25|\n",
      "|  10004|     contact.html|            20.0|2023-07-25|\n",
      "|  10005|    homepage.html|            10.0|2023-07-25|\n",
      "|  10006|product_page.html|            95.0|2023-07-25|\n",
      "|  10007|        blog.html|           150.0|2023-07-26|\n",
      "|  10008|    homepage.html|            25.0|2023-07-26|\n",
      "|  10009|product_page.html|            85.0|2023-07-26|\n",
      "|  10010|    checkout.html|            50.0|2023-07-26|\n",
      "|  10011|         faq.html|            35.0|2023-07-27|\n",
      "|  10012|    homepage.html|            12.0|2023-07-27|\n",
      "|  10013|product_page.html|           110.0|2023-07-27|\n",
      "|  10014|    checkout.html|            60.0|2023-07-27|\n",
      "|  10015|        blog.html|           160.0|2023-07-28|\n",
      "|  10016|    homepage.html|            18.0|2023-07-28|\n",
      "|  10017|product_page.html|           100.0|2023-07-28|\n",
      "|  10018|    checkout.html|            55.0|2023-07-28|\n",
      "|  10019|     contact.html|            22.0|2023-07-29|\n",
      "|  10020|    homepage.html|            15.0|2023-07-29|\n",
      "+-------+-----------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- page_url: string (nullable = true)\n",
      " |-- session_duration: float (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa o Dataframe\n",
    "schema = StructType(\n",
    "    [\n",
    "          StructField(\"user_id\", StringType())\n",
    "         ,StructField(\"page_url\", StringType())\n",
    "         ,StructField(\"session_duration\", FloatType())\n",
    "         ,StructField(\"date\", DateType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_website_access = spark.read.csv(\"input/website_logs.csv\", header=True, schema=schema)\n",
    "df_website_access.show()\n",
    "df_website_access.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         page_url|count|\n",
      "+-----------------+-----+\n",
      "|    homepage.html|   51|\n",
      "|product_page.html|   46|\n",
      "|    checkout.html|   35|\n",
      "|     contact.html|   19|\n",
      "|        blog.html|   18|\n",
      "|         faq.html|   14|\n",
      "|    about_us.html|    1|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 - Identifique as 10 páginas mais visitadas.\n",
    "\n",
    "df_website_access.groupby(\"page_url\")  \\\n",
    "    .count()                 \\\n",
    "    .sort(f.col(\"count\")\n",
    "          .desc())              \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|Duração média das sessões|\n",
      "+-------------------------+\n",
      "|        60.84239130434783|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Calcule a média de duração das sessões dos usuários.\n",
    "\n",
    "df_website_access.select(f.avg(\"session_duration\").alias(\"Duração média das sessões\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 usuários retornam ao site mais de uma vez por semana.\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Determine quantos usuários retornam ao site mais de uma vez por semana.\n",
    "\n",
    "# Cria uma coluna que identifica a semana e o ano\n",
    "df_website_access = df_website_access.withColumn(\"year_week\",f.concat_ws(\"-\",f.year(\"date\"),f.weekofyear(\"date\")))\n",
    "\n",
    "# Cria um Dataframe com os usuários que retornam com mais de 1 acesso semanal\n",
    "df_weekly_website_access = df_website_access.groupBy(\"user_id\",\"year_week\").count().filter(\"count > 1\")\n",
    "\n",
    "df_returning_users = df_weekly_website_access.select(\"user_id\").distinct().count()\n",
    "output_message = f\"{df_returning_users} usuários retornam ao site mais de uma vez por semana.\"\n",
    "print(output_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Desenho de Arquitetura\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "Proponha uma arquitetura em AWS para coletar dados de diferentes fontes:\n",
    "\n",
    "- Desenhe um sistema para coletar dados de uma API.\n",
    "- Processe esses dados em tempo real.\n",
    "- Armazene os dados para análise futura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagrams import Cluster, Diagram\n",
    "from diagrams.aws.compute import Lambda\n",
    "from diagrams.aws.network import APIGateway\n",
    "from diagrams.aws.analytics import KinesisDataStreams, Redshift\n",
    "from diagrams.aws.storage import S3\n",
    "\n",
    "with Diagram(\"Coleta e Processamento de Dados\", show=False):\n",
    "    with Cluster(\"Coleta de Dados\"):\n",
    "        api_gateway = APIGateway(\"API Gateway\")\n",
    "        data_source = api_gateway >> Lambda(\"Processador de Dados\")\n",
    "        \n",
    "    with Cluster(\"Processamento em Tempo Real\"):\n",
    "        kinesis_stream = KinesisDataStreams(\"Kinesis Stream\")\n",
    "        data_processor = Lambda(\"Processamento\\nem Tempo Real\")\n",
    "        data_source >> data_processor >> kinesis_stream\n",
    "\n",
    "    with Cluster(\"Armazenamento e Análise\"):\n",
    "        s3_bucket = S3(\"Bucket S3\")\n",
    "        redshift = Redshift(\"Amazon Redshift\")\n",
    "        kinesis_stream >> s3_bucket\n",
    "        s3_bucket >> redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Codificação\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "Escreva um script Python para:\n",
    "\n",
    "- Se conectar à [API de previsão do tempo OpenWeatherMap](https://openweathermap.org/api).\n",
    "- Coletar dados dessa API para uma cidade de sua escolha.\n",
    "- Armazenar os dados coletados em um banco de dados relacional ou NoSQL(de sua escolha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'EKEAD0MWKWS5RQH3',\n",
       "  'HostId': 'H2PniT0TTVHFg7kOpNVchZhTqP+uDYFneF8Wlj+wby1TEBEWh3Ng0KklrV9Z6L6Eep3DMBnKYgk=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'H2PniT0TTVHFg7kOpNVchZhTqP+uDYFneF8Wlj+wby1TEBEWh3Ng0KklrV9Z6L6Eep3DMBnKYgk=',\n",
       "   'x-amz-request-id': 'EKEAD0MWKWS5RQH3',\n",
       "   'date': 'Mon, 07 Aug 2023 16:18:29 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"e0ea2be95da9486bb53c8135ee14d51c\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"e0ea2be95da9486bb53c8135ee14d51c\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_KEY = getenv(\"OPEN_WHEATER_API_KEY\")\n",
    "cidade = \"miami\"\n",
    "link = f\"https://api.openweathermap.org/data/2.5/weather?q={cidade}&appid={API_KEY}&lang=pt_br\"\n",
    "\n",
    "with req.get(link) as r:\n",
    "    if r.status_code != 200:\n",
    "        print(\"Erro ao fazer requisição: \",r.status_code)\n",
    "    response = r.content\n",
    "\n",
    "# descricao = response_dic['weather'][0]['description']\n",
    "# temperatura = response_dic['main']['temp'] - 273.15\n",
    "# print(descricao, f\"{temperatura}ºC\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3'\n",
    "    , aws_access_key_id=aws_access_key_id\n",
    "    , aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "bucket_name = 'bucket-teste-iam'\n",
    "file_name = 'open_weather_miami.json'\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=file_name, Body=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality & Observability\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "A qualidade dos dados é fundamental para garantir que as análises e os insights derivados sejam confiáveis. Observabilidade, por outro lado, refere-se à capacidade de monitorar e entender o comportamento dos sistemas. Para este desafio:\n",
    "\n",
    "- Utilize o arquivo `sales_data.csv` e implemente verificações de qualidade de dados. Por exemplo:\n",
    "  - Verifique se todos os IDs de usuários são únicos.\n",
    "  - Confirme se os valores de vendas não são negativos.\n",
    "  - Garanta que todas as entradas tenham timestamps válidos.\n",
    "  - Quantidade de linhas ingeridas no banco de dados de sua escolha é igual a quantidade de linhas originais\n",
    "  \n",
    "- Crie métricas de observabilidade para o processo ETL que você desenvolveu anteriormente(não é necessário implementação):\n",
    "  - Monitore o tempo que leva para os dados serem extraídos, transformados e carregados.\n",
    "  - Implemente alertas para qualquer falha ou anomalia durante o processo ETL.\n",
    "  - Descreva como você rastrearia um problema no pipeline, desde o alerta até a fonte do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs de usuários são únicos.\n",
      "Valores de vendas não são negativos.\n",
      "Timestamps válidos.\n",
      "Quantidade de linhas após ETL é igual à quantidade de linhas originais.\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Utilize o arquivo `sales_data.csv` e implemente verificações de qualidade de dados.\n",
    "\n",
    "#   - Verifique se todos os IDs de usuários são únicos.\n",
    "if df_sales_raw['transaction_id'].nunique() == df_sales_raw.shape[0]:\n",
    "    print(\"IDs de usuários são únicos.\")\n",
    "else:\n",
    "    print(\"IDs de usuários não são únicos.\")\n",
    "\n",
    "#   - Confirme se os valores de vendas não são negativos.\n",
    "if (df_sales_raw['sale_value'] >= 0).all():\n",
    "    print(\"Valores de vendas não são negativos.\")\n",
    "else:\n",
    "    print(\"Existem valores de vendas negativos.\")\n",
    "\n",
    "#   - Garanta que todas as entradas tenham timestamps válidos.\n",
    "try:\n",
    "    df_sales_raw['date'] = pd.to_datetime(df_sales_raw['date'])\n",
    "    print(\"Timestamps válidos.\")\n",
    "except:\n",
    "    print(\"Existem timestamps inválidos.\")\n",
    "\n",
    "#   - Quantidade de linhas ingeridas no banco de dados de sua escolha é igual a quantidade de linhas originais\n",
    "if df_sales_refined.shape[0] == df_sales_raw.shape[0]:\n",
    "    print(\"Quantidade de linhas após ETL é igual à quantidade de linhas originais.\")\n",
    "else:\n",
    "    print(\"Quantidade de linhas após ETL não é igual à quantidade de linhas originais.\")\n",
    "\n",
    "# Monitorando o tempo do processo ETL e implementando alertas\n",
    "# Você pode usar bibliotecas como time ou datetime para medir o tempo e enviar alertas\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "#ETL\n",
    "df_sales = pd.read_csv('input/sales_data.csv')\n",
    "df_sales = df_sales.drop_duplicates()\n",
    "df_sales = df_sales.fillna(0)\n",
    "df_sales['usd_sale_value'] = df_sales['sale_value'] * 0.75\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# if execution_time > YOUR_THRESHOLD:\n",
    "#     send_alert(\"Tempo de execução do ETL excedeu o limite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Teste Unitário\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "Os testes unitários são fundamentais para garantir a robustez e confiabilidade do código, permitindo identificar e corrigir bugs e erros antes que eles atinjam o ambiente de produção. Para este desafio:\n",
    "\n",
    "- Escolha uma das funções ou classes que você implementou nas etapas anteriores deste teste.\n",
    "- Escreva testes unitários para esta função ou classe. Os testes devem cobrir:\n",
    "  - Casos padrão ou \"happy path\".\n",
    "  - Casos de borda ou extremos.\n",
    "  - Situações de erro ou exceção.\n",
    "- Utilize uma biblioteca de testes de sua escolha (como `pytest`, `unittest`, etc.).\n",
    "- Documente os resultados dos testes e, caso encontre falhas através dos testes, descreva como as corrigiria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_empty_dataframe (__main__.TestCheckSalesNonNegative) ... ok\n",
      "test_missing_column (__main__.TestCheckSalesNonNegative) ... ok\n",
      "test_mixed_values (__main__.TestCheckSalesNonNegative) ... FAIL\n",
      "test_positive_values (__main__.TestCheckSalesNonNegative) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_mixed_values (__main__.TestCheckSalesNonNegative)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46192/3745988522.py\", line 15, in test_mixed_values\n",
      "    self.assertFalse(check_sales_non_negative(df_mixed))\n",
      "AssertionError: True is not false\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.015s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb82a6ce8c0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para verificar se os valores de vendas não são negativos\n",
    "def check_sales_non_negative(df):\n",
    "    return (df['sales'] >= 0).all()\n",
    "\n",
    "# Testes unitários usando o módulo unittest\n",
    "class TestCheckSalesNonNegative(unittest.TestCase):\n",
    "    def test_positive_values(self):\n",
    "        # Caso padrão (todos os valores são não negativos)\n",
    "        df_positive = pd.DataFrame({'sales': [100, 200, 150]})\n",
    "        self.assertTrue(check_sales_non_negative(df_positive))\n",
    "\n",
    "    def test_mixed_values(self):\n",
    "        # Caso de borda (alguns valores negativos)\n",
    "        df_mixed = pd.DataFrame({'sales': [50, 20, 300]})\n",
    "        self.assertFalse(check_sales_non_negative(df_mixed))\n",
    "\n",
    "    def test_missing_column(self):\n",
    "        # Caso de erro (coluna 'sales' ausente)\n",
    "        df_missing_column = pd.DataFrame({'revenue': [100, 200, 150]})\n",
    "        with self.assertRaises(KeyError):\n",
    "            check_sales_non_negative(df_missing_column)\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        # Caso de erro (DataFrame vazio)\n",
    "        df_empty = pd.DataFrame({'sales': []})\n",
    "        self.assertTrue(check_sales_non_negative(df_empty))\n",
    "\n",
    "# Executar os testes\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
